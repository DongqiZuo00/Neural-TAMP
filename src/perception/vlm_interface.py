import sys
import os

# è·å–å½“å‰è„šæœ¬çš„ç›®å½•: .../Neural-TAMP/src/perception
script_dir = os.path.dirname(os.path.abspath(__file__))
# è·å–é¡¹ç›®æ ¹ç›®å½•: .../Neural-TAMP
project_root = os.path.abspath(os.path.join(script_dir, "../.."))

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥è·¯å¾„ï¼Œè¿™æ · Python å°±èƒ½æ‰¾åˆ° 'src' åŒ…äº†
sys.path.append(project_root)
import torch
import json
import re
import uuid
from PIL import Image
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# å¼•å…¥æˆ‘ä»¬åˆšæ‰å®šä¹‰çš„é€šç”¨æ•°æ®ç»“æ„
from src.core.graph_schema import SceneGraph, Node, Edge

class VLMInterface:
    def __init__(self, model_path="Neural-TAMP/models/Qwen2-VL-7B-Instruct", device="cuda"):
        print(f"[VLM] Initializing Qwen2-VL from {model_path}...")
        self.device = device
        
        try:
            # åŠ è½½æ¨¡å‹ (è‡ªåŠ¨ä½¿ç”¨ bfloat16 èŠ‚çœæ˜¾å­˜)
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto"
            )
            self.processor = AutoProcessor.from_pretrained(model_path)
            print("[VLM] Model loaded successfully.")
        except Exception as e:
            print(f"[VLM Error] Failed to load model: {e}")
            print("Tip: Check if the model path is correct and download finished.")
            raise e

    def _build_system_prompt(self, instruction: str) -> str:
        """
        [Prompt Engineering] 
        è¿™æ˜¯æ•´ä¸ªæ„ŸçŸ¥æ¨¡å—æœ€å…³é”®çš„éƒ¨åˆ†ã€‚
        æˆ‘ä»¬å¿…é¡»ç»™ VLM ä¸€ä¸ªä¸¥æ ¼çš„ Schemaï¼Œå¦åˆ™å®ƒä¼šå¼€å§‹è®²æ•…äº‹ã€‚
        """
        return f"""You are a robot perception system. 
        Task: Analyze the image based on the instruction: "{instruction}".
        Output: A Scene Graph in strict JSON format.
        
        **Requirements:**
        1. **Objects**: Detect the target object, receptacle, and any obstacles/blockers.
        2. **Boxes**: Provide 2D bounding boxes [ymin, xmin, ymax, xmax] (scale 0-1000).
        3. **States**: Infer states like "Open", "Closed", "Empty", "Full".
        4. **Relations**: Identify spatial relations ("inside", "on", "close_to") and logical relations ("blocked_by").
        
        **Output Format (JSON Only):**
        ```json
        {{
          "objects": [
            {{ "label": "apple", "box_2d": [100, 200, 300, 400], "state": "default" }},
            {{ "label": "fridge", "box_2d": [0, 500, 1000, 900], "state": "closed" }}
          ],
          "relations": [
            {{ "source_label": "apple", "target_label": "fridge", "relation": "inside" }}
          ]
        }}
        ```
        Do not output any markdown or explanation. Just the JSON string. """
        
    def parse(self, image_input, instruction: str) -> SceneGraph:
        """
        æ ¸å¿ƒæµç¨‹: Image -> VLM -> Text -> JSON -> SceneGraph Object
        """
        # 1. å›¾åƒåŠ è½½
        if isinstance(image_input, str):
            image = Image.open(image_input)
        else:
            image = image_input
    
        # 2. æ„å»ºè¾“å…¥
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": self._build_system_prompt(instruction)},
                ],
            }
        ]
    
        # 3. é¢„å¤„ç†
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        ).to(self.device)
    
        # 4. æ¨ç†
        with torch.no_grad():
            generated_ids = self.model.generate(**inputs, max_new_tokens=1024)
            
        # 5. è§£ç 
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]
    
        # 6. è§£æå¹¶è½¬æ¢ä¸º Graph å¯¹è±¡
        return self._text_to_graph(output_text)

    def _text_to_graph(self, text: str) -> SceneGraph:
        """
        å°† VLM çš„æ–‡æœ¬è¾“å‡ºæ¸…æ´—å¹¶å°è£…æˆ SceneGraph å¯¹è±¡
        """
        sg = SceneGraph()
    
        try:
            # æå– JSON å—
            match = re.search(r"```json(.*?)```", text, re.DOTALL)
            if match:
                json_str = match.group(1).strip()
            else:
                # å°è¯•ç›´æ¥æ‰¾ {}
                match = re.search(r"\{.*\}", text, re.DOTALL)
                json_str = match.group(0).strip() if match else "{}"
            
            data = json.loads(json_str)
            
            # --- æ„å»º Node ---
            # è¿™é‡Œçš„ ID åªæ˜¯ä¸´æ—¶çš„ï¼ŒGlobal Fusion æ—¶ä¼šæ›´æ–°
            label_to_id = {} 
            
            for i, obj in enumerate(data.get("objects", [])):
                label = obj.get("label", "unknown")
                # ç”Ÿæˆå”¯ä¸€ ID: label + åºå· (e.g., "apple|0")
                node_id = f"{label}|{i}" 
                label_to_id[label] = node_id # ç®€å•è®°å½•ï¼Œç”¨äºå¤„ç†è¾¹çš„å…³ç³»
                
                # åˆ›å»º Node å¯¹è±¡ (æ³¨æ„ï¼šç›®å‰ pos è¿˜æ˜¯ç©ºçš„ï¼Œspatial_lifter è´Ÿè´£å¡«å…¥)
                node = Node(
                    id=node_id,
                    label=label,
                    pos=(0.0, 0.0, 0.0), # å ä½ï¼Œå¾… depth å¡«å……
                    bbox=obj.get("box_2d", [0,0,0,0]), # è¿™é‡Œå­˜çš„æ˜¯ 2D æ¡†
                    state=obj.get("state")
                )
                sg.add_node(node)
                
            # --- æ„å»º Edge ---
            for rel in data.get("relations", []):
                src_label = rel.get("source_label")
                tgt_label = rel.get("target_label")
                relation = rel.get("relation")
                
                # å°è¯•æ‰¾åˆ°å¯¹åº”çš„ ID (ç®€å•çš„æ¨¡ç³ŠåŒ¹é…)
                # å®é™…ç”Ÿäº§ä¸­è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„åŒ¹é…é€»è¾‘ï¼Œé˜²æ­¢å¤šä¸ª apple ææ··
                src_id = None
                tgt_id = None
                
                for l, nid in label_to_id.items():
                    if src_label in l: src_id = nid
                    if tgt_label in l: tgt_id = nid
                
                if src_id and tgt_id:
                    edge = Edge(src_id, tgt_id, relation)
                    sg.add_edge(edge)
                    
        except Exception as e:
            print(f"[VLM Parser Error] Could not parse JSON: {e}")
            print(f"[Raw Output] {text}")
            
        return sg

# ä¿®æ”¹ src/perception/vlm_interface.py çš„åº•éƒ¨

if __name__ == "__main__":
    import os
    
    # 1. æŒ‡å®šå›¾ç‰‡è·¯å¾„
    img_path = "Neural-TAMP/test_image.png" # ç¡®ä¿ä½ æŠŠå›¾ç‰‡æ”¾åˆ°äº†è¿™é‡Œ
    
    # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œå…ˆåˆ›å»ºä¸€ä¸ªå‡çš„ï¼ˆé˜²æ­¢æŠ¥é”™ï¼Œä½†æœ€å¥½ç”¨ä½ çš„çœŸå›¾ï¼‰
    if not os.path.exists(img_path):
        print(f"Warning: {img_path} not found. using dummy white image.")
        dummy_img = Image.new('RGB', (640, 480), color='white')
        image_input = dummy_img
    else:
        print(f"Loading real image from {img_path}...")
        image_input = img_path

    # 2. åˆå§‹åŒ–æ¥å£
    try:
        vlm = VLMInterface()
        
        # 3. å‘é€æŒ‡ä»¤ (æˆ‘ä»¬æ•…æ„é—®ä¸€ä¸ªç¨å¾®éš¾ç‚¹çš„)
        # æŒ‡ä»¤ï¼šæ‰¾åˆ°æ‰€æœ‰ç‰©ä½“ï¼Œç‰¹åˆ«æ˜¯æ¤ç‰©
        instruction = "Detect all objects in the room, especially the plant."
        
        print(f"Instruction: {instruction}")
        print("Parsing...")
        
        graph = vlm.parse(image_input, instruction)
        
        print("\n" + "="*40)
        print("ğŸ‰ SUCCESS! Generated Scene Graph:")
        print("="*40)
        # æ‰“å°ç”Ÿæˆçš„ Prompt æ–‡æœ¬ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å« "Plant"
        print(graph.to_prompt_text()) 
        
        print("\n[Raw Node Data]:")
        for node_id, node in graph.nodes.items():
            print(f"- {node.label} (Box: {node.bbox})")
            
    except Exception as e:
        print(f"\nâŒ Test Failed: {e}")
        import traceback
        traceback.print_exc()
