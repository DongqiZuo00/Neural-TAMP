import sys
import os
import random
import shutil
import json
import time

# --- è·¯å¾„ä¿®æ­£ (é˜²æ­¢ ModuleNotFoundError) ---
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from src.env.procthor_wrapper import ProcTHOREnv
from src.memory.graph_manager import GraphManager
from src.utils.visualizer import BEVVisualizer
from src.perception.oracle_interface import OracleInterface
from src.task.task_generator import TaskGenerator

# [å…³é”®ä¿®æ”¹] ä½¿ç”¨æ–°çš„åˆ†å±‚è§„åˆ’å™¨ (MLDT Logic)
from src.planning.decomposer import TaskDecomposer 

def main():
    print("="*60)
    print("ğŸš€ Neural-TAMP: Batch Generation Pipeline (MLDT Planner Integration)")
    print("="*60)

    # 0. æ£€æŸ¥ API Key
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  WARNING: OPENAI_API_KEY is not set. The Planner will likely fail.")
        # ä½ å¯ä»¥åœ¨è¿™é‡Œé€‰æ‹© returnï¼Œæˆ–è€…è®©å®ƒæŠ¥é”™
        # return 

    # 1. å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = "Neural-TAMP/vis_output/batch_dataset"
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir) # æ¸…ç©ºæ—§æ•°æ®
    os.makedirs(output_dir)
    print(f"ğŸ“‚ Output Directory: {output_dir}")

    # 2. åˆå§‹åŒ–æ‰€æœ‰ AI æ¨¡å—
    try:
        print("[System] Initializing Modules...")
        env = ProcTHOREnv()
        oracle = OracleInterface(env)
        memory = GraphManager(save_dir="Neural-TAMP/memory_data")
        viz = BEVVisualizer(save_dir=output_dir)
        task_gen = TaskGenerator()
        
        # ä½¿ç”¨æ–°çš„åˆ†å±‚è§„åˆ’å™¨
        planner = TaskDecomposer(model_name="gpt-4o") 
        
        print("âœ… All Systems Ready.")
    except Exception as e:
        print(f"âŒ Initialization Failed: {e}")
        return

    # 3. é…ç½®æ‰¹é‡ç”Ÿæˆå‚æ•°
    TOTAL_SAMPLES = 50  # ç›®æ ‡æ ·æœ¬æ•°
    count = 0
    # éšæœºæŠ½å– 200 ä¸ªå¤‡é€‰åœºæ™¯ index
    candidate_indices = random.sample(range(10000), 200)
    
    # æ•°æ®é›†æ—¥å¿—åˆ—è¡¨
    dataset_log = []

    print("\nğŸ¬ Starting Data Generation Loop...")
    
    start_time = time.time()

    for idx in candidate_indices:
        if count >= TOTAL_SAMPLES:
            break

        print(f"\n" + "-"*40)
        print(f"ğŸ”„ Processing Candidate Index: {idx}")

        # --- A. åœºæ™¯åŠ è½½ (Environment) ---
        try:
            obs = env.change_scene(idx)
        except Exception as e:
            print(f"   âš ï¸ Scene Load Failed: {e}")
            continue

        # --- B. åœºæ™¯ç­›é€‰ (Filtering) ---
        # åªä¿ç•™å¤šæˆ¿é—´çš„å¤§æˆ·å‹ï¼Œä¿è¯å¯¼èˆªä»»åŠ¡çš„å¤æ‚åº¦
        num_rooms = len(env.current_scene.get("rooms", []))
        if num_rooms < 2:
            print(f"   âš ï¸ Skipped: Single Room Layout ({num_rooms} room)")
            continue 

        # --- C. æ„ŸçŸ¥ä¸è®°å¿†æ„å»º (Perception & Memory) ---
        # 1. Oracle è·å–å¸¦å‡ ä½•ä¿¡æ¯å’Œ Room ID çš„ Graph
        hierarchical_graph = oracle.get_hierarchical_graph()
        
        # 2. å­˜å…¥è®°å¿†ï¼Œå¹¶è®¡ç®—ä¸¥æ ¼çš„ "Same-Room" Edge
        memory.override_global_graph(hierarchical_graph)
        
        # --- D. ä»»åŠ¡ç”Ÿæˆ (Task Generation) ---
        # åŸºäºå½“å‰çš„ Graph ç”Ÿæˆä¸€ä¸ªå¯è¡Œçš„ Pick & Place ä»»åŠ¡
        instruction, task_meta = task_gen.generate(memory.global_graph)
        
        if instruction is None:
            print(f"   âš ï¸ Task Gen Failed: {task_meta.get('error')}")
            continue

        print(f"   âœ… Task Generated: \"{instruction}\"")

        # --- E. [æ ¸å¿ƒ] åˆ†å±‚è§„åˆ’ (Hierarchical Planning) ---
        # ä½¿ç”¨ TaskDecomposer: Task -> Subgoals -> Actions
        try:
            plan_actions = planner.plan(instruction, memory.global_graph)
        except Exception as e:
            print(f"   âŒ Planning Exception: {e}")
            plan_actions = []

        if not plan_actions:
            print("   âš ï¸ Planner returned empty plan. Skipping sample.")
            continue

        # --- F. æ•°æ®ä¿å­˜ (Data Saving) ---
        # 1. ä¿å­˜å›¾ç‰‡ (GT vs AI Perception)
        gt_filename = f"scene_{idx:05d}_GT.png"
        ai_filename = f"scene_{idx:05d}_AI.png"
        
        env.save_ground_truth_bev(os.path.join(output_dir, gt_filename))
        viz.render(memory.global_graph, filename=ai_filename)

        # 2. æ„é€ æ•°æ®æ¡ç›®
        log_entry = {
            "scene_index": idx,
            "num_rooms": num_rooms,
            "instruction": instruction,
            "task_metadata": task_meta,     # åŒ…å« target_id, dest_idï¼Œæ–¹ä¾¿ GNN è®­ç»ƒ
            "plan": plan_actions,           # è¿™æ˜¯ä½ çš„ Ground Truth Actions
            "visualization": {
                "ground_truth_bev": gt_filename,
                "ai_semantic_map": ai_filename
            }
            # å¦‚æœéœ€è¦ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥ä¿å­˜ memory.global_graph.to_dict()
        }
        dataset_log.append(log_entry)
        count += 1
        
        print(f"   ğŸ’¾ Sample Saved. Total: {count}/{TOTAL_SAMPLES}")

        # å®æ—¶å†™å…¥ JSONï¼Œé˜²æ­¢ä¸­é€”å´©æºƒæ•°æ®ä¸¢å¤±
        with open(os.path.join(output_dir, "dataset_full.json"), "w") as f:
            json.dump(dataset_log, f, indent=2)

    # 4. ç»“æŸä¸ç»Ÿè®¡
    env.stop()
    duration = time.time() - start_time
    print("\n" + "="*60)
    print(f"ğŸ‰ Pipeline Finished in {duration:.1f}s.")
    print(f"âœ… Successfully collected {count} samples.")
    print(f"ğŸ“‚ Dataset location: {os.path.join(output_dir, 'dataset_full.json')}")
    print("="*60)

if __name__ == "__main__":
    main()